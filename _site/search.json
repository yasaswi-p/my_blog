[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Finley Malloc",
    "section": "",
    "text": "TESTING QUARTO Finley Malloc is the Chief Data Scientist at Wengo Analytics. When not innovating on data platforms, Finley enjoys spending time unicycling and playing with her pet iguana."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Finley Malloc",
    "section": "Education",
    "text": "Education\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St.Â Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Finley Malloc",
    "section": "Experience",
    "text": "Experience\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Spet 2012 - April 2018"
  },
  {
    "objectID": "Mahabharata/mahabharata-analysis.html",
    "href": "Mahabharata/mahabharata-analysis.html",
    "title": "my_blog",
    "section": "",
    "text": "import re\nimport os\nimport nltk\nimport spacy\nimport string\nimport pprint\nimport random\nimport operator\n# import powerlaw\nimport numpy as np\nimport collections\nimport pandas as pd\nimport numpy as np \nimport helper2 as h2\nimport networkx as nx\nimport seaborn as sns\n# import pygraphviz as pgv\nimport matplotlib.cm as cm\nfrom pylab import rcParams\nfrom itertools import count\nfrom itertools import islice\nfrom tabulate import tabulate\nfrom textblob import TextBlob\nfrom nltk.stem.porter import *\nfrom wordcloud import WordCloud\nfrom operator import itemgetter\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nimport matplotlib.colors as mcolors\nfrom numpy.random import default_rng\nfrom matplotlib import pyplot, patches \nfrom nltk.tokenize import word_tokenize\nfrom timeit import default_timer as timer\nfrom collections import defaultdict, Counter\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse.csgraph import floyd_warshall, dijkstra\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nstemmer = PorterStemmer()\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\n\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n</style>\n\"\"\")\n\n\n\n\n\n\n\nimport nltk\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('sentiwordnet')\nnltk.download('stopwords')\n\n[nltk_data] Downloading package punkt to /home/yasaswi/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /home/yasaswi/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package wordnet to /home/yasaswi/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package sentiwordnet to\n[nltk_data]     /home/yasaswi/nltk_data...\n[nltk_data]   Package sentiwordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /home/yasaswi/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\n\nTrue\n\n\n\nfrom spacy.lang.en.stop_words import STOP_WORDS as scipy_stop\nfrom nltk.corpus import stopwords as nltk_stop \nfrom wordcloud import STOPWORDS as wc_stop\n\nstop_words_list = []\nstop_words_list.extend(list(scipy_stop))\nstop_words_list.extend(list(nltk_stop.words('english')))\nstop_words_list.extend(list(wc_stop))\n\n# List of custom stop words \ncustom_stop_words = [\n    'well', 'would', 'never', 'latitude', 'longitude', 'wonderland', 'wonder', 'adventure', 'adventures', 'chapter',\n    'car','race','great','please', 'maam', 'drink', 'think', 'sink', 'come', 'foot', 'right', 'thats', 'too', 'itll', \n    'tell', 'table', 'long', 'ye', 'let', 'pandavas', 'tale', 'test', 'said', 'held', 'crab', 'next','men', 'sure', \n    'digging', 'i', 'i\\'ve', 'oh', 'bills','battle','islandborn', 'number','thou','thy', 'unto','ill', 'im', 'ive',\n    'whats', 'read', 'little', 'id',    'look', 'dont', 'luckily', 'get', 'lizard', 'the', 'son', 'addressed', \n    'continued', 'thee', 'viz', 'diademdecked', 'king', 'word','words', 'beholding', 'sons', 'hath', 'great', 'brahmana',\n    'kuru', 'hear', 'slay', 'grandsire', 'krishnadwaipayana']\n\nstop_words_list = list(set(stop_words_list))\nstop_words_list += custom_stop_words\n\n\ndef read_file(filename, remove_stop_words = True, preprocess = True, return_type = list):\n    \"\"\"\n    This function reads content from given file and does preprocessing\n    and stop words removal as decided by the parameters.\n\n    Parameters\n    ----------\n    filename : str\n      Path to the file location\n\n    remove_stop_words : bool \n      A boolean value that dictates if stop words should be removed.\n\n    preprocess : bool\n      A boolean value that dictates if preprocessing should be done.\n      Characters like carriage return, linefeed, form feed and punctuation \n      will be removed\n\n    return_type : type\n       Decides if the return is a list of sentences or a paragraph string\n       \n    Returns\n    ----------\n    contents : list or str\n       Returns preprocessed contents either as a list of sentences\n       or as a whole string which is decided by return_type.\n    \"\"\"\n    contents = []\n    with open (filename, 'r' , encoding = 'utf-8') as f:\n        for l in f:\n            line = l.lower().strip()\n            if line.strip():\n                line = line.replace('\\n', ' ').replace('\\r', ' ').replace('\\'s', '').replace('\\'', '')\n                if preprocess == True:\n                    line = ''.join([char for char in line if char not in string.punctuation]) # Removing Punctuations\n                if remove_stop_words == True:\n                    line = ' '.join([word for word in line.split() if word.lower() not in stop_words_list])\n                for key_name, aliases in h2.same_names_dict.items():\n                    for alias in aliases:\n                        line = re.sub(r\"\\b{}\\b\".format(alias), key_name, line)\n                contents.append(line)    \n    if return_type == str:\n        return ' '.join(contents)\n    else:\n        return contents\n\n\ndef get_interactions(contents, character_list, N = 15):\n    \"\"\"\n    This function outputs a character interaction dictionary - a opmised version of a co-occurance matrix\n\n    Parameters\n    ----------\n    contents : str\n        Paragraph from which character interactions are to be extracted \n\n    character_list : bool \n        A boolean value that dictates if stop words should be removed.\n\n    N : bool\n        Decides the maximum distance(in words) between two character names \n        to consider a valid interaction\n\n    Returns\n    ----------\n    res : dict\n       Dictionary with character pairs as keys and no. of interactions as values\n    \"\"\"\n    res = defaultdict(int)\n    word_list = contents.split(' ')\n    for index, word in enumerate(word_list):\n        if word in character_list:\n            try:\n                interaction_list = word_list[index : index + N + 1]\n            except:\n                pdb.set_trace()\n            for character in character_list:    \n                if character!=word:\n                    if character in interaction_list:\n                        key = tuple(sorted([character, word]))\n                        res[key]+=1\n    return res\n\n\n# Co-occurance if interaction is present in 15 words\ndef get_graph_from_file(contents):    \n    \"\"\"\n    This function derives a network using character interaction/ co-occurance dictionary\n    based on the contents string provided\n\n    Parameters\n    ----------\n    contents : str\n        Paragraph from which character interactions are to be extracted \n        \n    Returns\n    ----------\n    G : graph\n          A NetworkX graph\n    \"\"\"\n    \n    character_list = h2.all_characters\n    res = get_interactions(contents, character_list, N = 15)\n    co_occurance_list = list([(*k,v) for (k,v) in res.items()])      \n    G=nx.Graph()\n    G.add_nodes_from(character_list)\n    G.add_weighted_edges_from(co_occurance_list)\n    isolates = list(nx.isolates(G))\n    G.remove_nodes_from(isolates)\n    return G\n\n\ndef get_top_n_central_nodes(centrality_dict, top_n = 5):\n    \"\"\"\n    This function returns the top n central nodes along\n    with their centrality values \n\n    Parameters\n    ----------\n    centrality_dict : dict\n        Dictionary with centrality values of characters\n    top_n : int\n        No of top cental nodes to be returned\n    \n    Returns\n    ----------\n    top_nodes : list\n          A list of top nodes and their centrality values\n    \"\"\"\n    top_nodes =  [str(k) + ': ' + str(round(v, 4)) for k, v in sorted(centrality_dict.items(), key = lambda x : x[1], \n                                                                reverse = True )[:top_n]]\n    return top_nodes\n\ndef get_top_n_central_nodes_without_values(centrality_dict, top_n = 5):\n    \"\"\"\n    This function returns the top n central nodes\n\n    Parameters\n    ----------\n    centrality_dict : dict\n        Dictionary with centrality values of characters\n    top_n : int\n        No of top cental nodes to be returned\n        \n    Returns\n    ----------\n    top_nodes : list\n          A list of top nodes without centrality values\n    \"\"\" \n    top_nodes =  [str(k) for k, v in sorted(centrality_dict.items(), key = lambda x : x[1], reverse = True )[:top_n]]\n    return top_nodes\n    \n\ndef get_central_actors(G, top_n = 5, with_values = False):\n    \"\"\"\n    This function gets the list of top n central nodes using \n    various centrality measures \n    Parameters\n    ----------\n    G : graph\n          A NetworkX graph\n    top_n : int\n        No of top cental nodes to be returned\n    with_values : bool\n        A boolean value that decides if the centrality values should also \n        be displayed        \n    \"\"\" \n    \n    numbers = range(1, top_n+1)\n    centralities = ['#', 'Betweenness', 'Degree', 'Pagerank', 'closeness']\n    eig_dict = nx.eigenvector_centrality(G)\n    deg_dict = nx.degree_centrality(G)\n    pag_dict = nx.pagerank(G)\n    bet_dict = nx.betweenness_centrality(G)\n    close_dict = nx.closeness_centrality(G)\n    if with_values == True:\n        bet_top = get_top_n_central_nodes(bet_dict, top_n)\n        deg_top = get_top_n_central_nodes(deg_dict, top_n)\n        pag_top = get_top_n_central_nodes(pag_dict, top_n) \n        close_top = get_top_n_central_nodes(close_dict, top_n)    \n        print(tabulate(np.c_[numbers, bet_top ,deg_top, pag_top, close_top], headers = centralities))\n    else:\n        bet_top = get_top_n_central_nodes_without_values(bet_dict, top_n)\n        deg_top = get_top_n_central_nodes_without_values(deg_dict, top_n)\n        pag_top = get_top_n_central_nodes_without_values(pag_dict, top_n) \n    #     eig_top = get_top_n_central_nodes(eig_dict, top_n)    \n        close_top = get_top_n_central_nodes_without_values(close_dict, top_n)    \n        print(tabulate(np.c_[numbers, bet_top ,deg_top, pag_top, close_top], headers = centralities))\n\ndef take(n, iterable):\n    \"Return first n items of the iterable as a list\"\n    return dict(islice(iterable, n))\n\n\nfrom pathlib import Path\n\nfile_path = Path.cwd() / \"Mahabharata/Mahabharata-K.M.Ganguli.txt\"\n\n\ncontents = read_file(file_path, remove_stop_words = True, preprocess = True, return_type = str)    \nGx = get_graph_from_file(contents)\n\n\nLet us look at the basic statistics of the Graph that we have just created\n\nprint(f\"No of actors(nodes) in the Graph are {len(Gx.nodes)}\")\nprint(f\"No of ties(edges) in the Graph are {len(Gx.edges)}\")\nprint(f\"Avg Shortest Path Length is {nx.average_shortest_path_length(Gx)}\")\nprint(f\"Avg Clustering Coefficient is {nx.average_clustering(Gx)}\")\nprint(f\"Transitivity of the Graph is {nx.transitivity(Gx)}\")\nprint(f\"Avg Node Connectivity is {nx.average_node_connectivity(Gx)}\")\nprint(f\"Max. distance b/w any pair of nodes (Diameter) is {nx.diameter(Gx)}\")\nprint(f\"Radius of the Graph is {nx.radius(Gx)}\")\n\nNo of actors(nodes) in the Graph are 154\nNo of ties(edges) in the Graph are 2728\nAvg Shortest Path Length is 1.8354978354978355\nAvg Clustering Coefficient is 0.6742193417994761\nTransitivity of the Graph is 0.5279993758411515\nAvg Node Connectivity is 19.93472540531364\nMax. distance b/w any pair of nodes (Diameter) is 4\nRadius of the Graph is 2\n\n\n\nn = 10\nget_central_actors(Gx, top_n = n, with_values = False)\n\n  #  Betweenness    Degree        Pagerank       closeness\n---  -------------  ------------  -------------  ------------\n  1  krishna        krishna       krishna        krishna\n  2  yudhishthira   yudhishthira  arjuna         yudhishthira\n  3  arjuna         arjuna        yudhishthira   arjuna\n  4  karna          bhima         bhima          bhima\n  5  bhima          duryodhana    drona          duryodhana\n  6  duryodhana     karna         karna          karna\n  7  vaisampayana   bhishma       bhishma        bhishma\n  8  bhishma        kunti         duryodhana     kunti\n  9  kunti          drona         dhritarashtra  drona\n 10  drona          vaisampayana  vaisampayana   vaisampayana\n\n\n\nn = 10\nget_central_actors(Gx, top_n = n, with_values = True)\n\n  #  Betweenness           Degree                Pagerank              closeness\n---  --------------------  --------------------  --------------------  --------------------\n  1  krishna: 0.1271       krishna: 0.8627       krishna: 0.0249       krishna: 0.8743\n  2  yudhishthira: 0.0659  yudhishthira: 0.7582  yudhishthira: 0.0206  yudhishthira: 0.801\n  3  arjuna: 0.0552        arjuna: 0.7582        arjuna: 0.0199        arjuna: 0.801\n  4  karna: 0.0391         bhima: 0.6863         bhima: 0.0177         bhima: 0.7574\n  5  bhima: 0.0358         duryodhana: 0.6863    duryodhana: 0.0177    duryodhana: 0.7537\n  6  duryodhana: 0.0328    karna: 0.6601         karna: 0.0171         karna: 0.7391\n  7  vaisampayana: 0.0289  bhishma: 0.6471       bhishma: 0.0166       bhishma: 0.7356\n  8  bhishma: 0.028        kunti: 0.6275         kunti: 0.0161         kunti: 0.7251\n  9  kunti: 0.0263         drona: 0.6144         vaisampayana: 0.0161  drona: 0.7183\n 10  drona: 0.024          vaisampayana: 0.6144  drona: 0.0156         vaisampayana: 0.7183\n\n\n\n# from pyvis.network import Network\n# net = Network(notebook = True, width=\"1000px\", height=\"700px\", bgcolor='#222222', font_color='white')\n\n# node_degree = dict(Gx.degree)\n\n# #Setting up node size attribute\n# nx.set_node_attributes(Gx, node_degree, 'size')\n\n# net.from_nx(Gx)\n# net.show(\"witcher.html\")\n\n\n# def draw(G, pos, measure, measure_name, color):\n#     \"\"\"\n#     This function draws a network based on a specific metric in the color and a specified layout\n\n#     Parameters\n#     ----------\n#     G : graph\n#       A NetworkX graph\n\n#     pos : dictionary, \n#       A dictionary with nodes as keys and positions as values.\n\n#     measure : dictionary\n#       Results of the centrality measure\n\n#     measure_name: str\n#        Name of the centrality measure used \n#     \"\"\"\n\n#     # Get the influencer node from measure\n#     influencer_node_index = max(enumerate(measure.values()), key = operator.itemgetter(1))[0]\n#     influencer_node  = list(measure.keys())[influencer_node_index]\n\n#     # Plot the graph\n#     plt.figure(figsize=(180, 180))\n#     # Draw the nodes\n#     nodes = nx.draw_networkx_nodes(G, pos, node_size = np.array(list(measure.values()))*80000, cmap = color,\n#                                    node_color = np.array(list(measure.values()))*80000, \n#                                    nodelist = list(measure.keys()), alpha = 0.4) \n#     # Draw Influencer Node with size = 800\n# #     nx.draw_networkx_nodes(G, pos, node_size = 800, node_color = 'darkGreen', nodelist= [influencer_node]) \n    \n#     # Draw the edges\n#     edges = nx.draw_networkx_edges(G, pos, edge_color = 'gray', alpha = 0.3)\n#     # Draw label only for the influencer node\n# #     nx.draw_networkx_labels(G, pos, font_size = 40, labels = {influencer_node: str(influencer_node)},font_color = 'red')\n#     nx.draw_networkx_labels(G, pos, font_size = 130, font_color = 'black')\n#     plt.title(measure_name, size = 80, weight = 'bold')\n#     plt.axis('off')\n#     plt.tight_layout()\n# #     plt.savefig('network_visualisation.jpg')\n#     plt.show()\n\n\n# page_rank = nx.degree_centrality(Gx)\n# page_rank_results = dict(sorted(page_rank.items(), key = lambda item: item[1], reverse = True))\n# pos = nx.spring_layout(Gx, k = 3)\n# draw(Gx, pos, page_rank_results, 'Degree Centrality', plt.cm.Reds)\n\n\n# page_rank = nx.pagerank(Gx, alpha = 0.85)\n# page_rank_results = dict(sorted(page_rank.items(), key = lambda item: item[1], reverse = True))\n# pos = nx.spring_layout(Gx, k = 2)\n# draw(Gx, pos, page_rank_results, 'Page Rank Centrality', plt.cm.Reds )\n\n\n\nCalculate and plot Ego network of top 15 characters in the Epic\n\ndef plot_influencers(G, N = 10):\n    \"\"\"\n    This function plots the top-n influencers in a bar chart \n    Parameters\n    ----------\n    G : graph\n          A NetworkX graph\n    N : int\n        No of influencers to be plotted\n    \"\"\"    \n    influence_dict = defaultdict(float)\n    for char in h2.all_characters:\n        if char in G.nodes:\n            influence_dict[char] = len(nx.ego_graph(G, n = char, radius = 1).nodes)\n    influence_dict = {k: v for k, v in sorted(influence_dict.items(), key=lambda item: item[1], reverse = True)}\n    plt.figure(figsize = (24, 15))\n    top_influences = take(N, influence_dict.items())\n    keys = [x.title() for x in top_influences.keys()]\n    vals = [top_influences[k.lower()] for k in keys]\n    percent = [round(float(top_influences[k.lower()]/len(influence_dict.keys()))*100,2) for k in keys]\n    labels = [f\"{str(vals[i])} ({str(percent[i])}%)\" for i in range(len(vals))]\n    labels = [f\" {str(percent[i])}%\" for i in range(len(vals))]\n\n    g = sns.barplot(x=keys, y=vals,palette = \"Set1\", alpha = 0.75)\n    g.bar_label(g.containers[0], labels = labels, size = 18)\n    g.tick_params(axis='x', which = 'major', labelsize = 25, rotation = 20)\n    g.tick_params(axis='y', which = 'major', labelsize = 25)\n    \n    plt.title('Ego Network of top 15 Central Characters in Mahabharata', size = 25, y = 1.05)\n    plt.tight_layout()\n    plt.savefig('ganguly_influencers.jpg')\n    plt.show()\n\nplot_influencers(Gx, 15)\n\n\n\n\n\ndef plot_centrality(G, N = 15):\n    \"\"\"\n    This function gets the list of top n central nodes using \n    various centrality measures \n    Parameters\n    ----------\n    G : graph\n          A NetworkX graph\n    N : int\n        No of influencers to be plotted\n    \"\"\"   \n    fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(25, 25))\n    fig.suptitle('Centrality Measures for Main Characters', size = 30, y = 1.05 )\n    ax = axes.flatten()\n    measures = [['Closeness Centrality', nx.closeness_centrality], \n                ['Betweenness Centrality', nx.betweenness_centrality],\n                ['Pagerank Centrality', nx.pagerank], \n                ['Degree Centrality',nx.degree_centrality]]\n    for i in range(len(measures)):\n        ax[i].set_title(measures[i][0], size = 50)\n        centrality_dict = measures[i][1](G)\n        centrality_dict = {k: v for k, v in sorted(centrality_dict.items(), key=lambda item: item[1], reverse = True)}\n        plt.figure(figsize = (24, 9))\n        top_influences = take(N, centrality_dict.items())\n        keys = [x.title() for x in top_influences.keys()]\n        vals = [top_influences[k.lower()] for k in keys]\n        g = sns.barplot(y=keys, x=vals , palette = \"Dark2\", alpha = 0.75, ax = ax[i])\n#         g.set_yticklabels(keys, size = 15)\n        g.tick_params(axis='both', which='major', labelsize = 40)\n\n#         g.set_xticklabels(g.get_xticks()[::2], size = 15)\n#         g.set_xticklabels([str(i) for i in g.get_xticks()], fontsize = 15)\n\n    fig.tight_layout()\n    fig.savefig('ganguly_central_characters.jpg', )\n    fig.show()\n\nplot_centrality(Gx, N = 5)\n\nUserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n<Figure size 1728x648 with 0 Axes>\n\n\n<Figure size 1728x648 with 0 Axes>\n\n\n<Figure size 1728x648 with 0 Axes>\n\n\n<Figure size 1728x648 with 0 Axes>\n\n\n\nfile_mb_summary_1 = Path.cwd() / \"Mahabharata/Mahabharata-C.Rajagopalachari.txt\"\nfile_mb_summary_2 = Path.cwd() / \"Mahabharata/Mahabharata-R.K.Narayan.txt\"\nfile_mb_full_1 = Path.cwd() / \"Mahabharata/Mahabharata-K.M.Ganguli.txt\"\nfile_mb_full_2 = Path.cwd() / \"Mahabharata/Mahabharata-Bibek_Debroy.txt\"\n\n\nmahabharata_sentences = read_file(file_mb_full_1, remove_stop_words = True, return_type = list)\n\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\ndef lemmatize_word(word):\n    \"\"\"\n    This function returns the lemmatized word based on pos-tagging\n    Parameters\n    ----------\n    word : str\n          word to be lemmatized\n    \n    Returns\n    ----------\n    lem_token : str\n          lemmatized word\n    \"\"\" \n    lemmatizer = WordNetLemmatizer()\n    lem_token = lemmatizer.lemmatize(word, pos=wordnet.NOUN)\n    # otherwise check for verb\n    if lem_token == word:\n        lem_token = lemmatizer.lemmatize(word, pos=wordnet.VERB)\n    # otherwise check for adjective\n    if lem_token == word:\n        lem_token = lemmatizer.lemmatize(word, pos=wordnet.ADJ)\n    # otherwiese check for adverb\n    if lem_token == word:\n        lem_token = lemmatizer.lemmatize(word, pos=wordnet.ADV)\n    return lem_token\n\n\nLet us plot word clouds to find out words which describe our top central characters\n\nmain_characters = ['krishna', 'arjuna', 'yudhishthira', 'duryodhana']\n\n\n# Extract all the sentences where the name of each character is mentioned\ncharacter_related_sentences = defaultdict(list)\nfor index,sentence in enumerate(mahabharata_sentences):\n    sentence = ' '.join([lemmatize_word(w) for w in sentence.split()])\n    for character in main_characters:\n        if character in sentence:\n                character_related_sentences[character].append(sentence)\n#                 character_related_sentences[character].append(mahabharata_sentences[index+1])\n#                 character_related_sentences[character].append(mahabharata_sentences[index+2])\n    character_related_sentences[character] = list(set(character_related_sentences[character]))\n\n\ndef plot_wordclouds(character_related_sentences, main_characters):\n    \"\"\"\n    This function plots word clouds of main characters\n    Parameters\n    ----------\n    character_related_sentences : dict\n          dictionary with keys as names of characters and values as list of sentences where the name\n          of the character is mentioned\n    main_characters : list\n          characters of interest whose words clouds are to be plotted\n    \"\"\" \n    for character in main_characters:\n        character_str = ' '.join(character_related_sentences[character])\n        wordcloud = WordCloud(width= 3000, height = 2000, random_state=1, \n                      background_color='black', colormap='Accent', \n                      collocations=False, stopwords = stop_words_list + h2.all_characters_unedited \n                              + h2.all_characters + custom_stop_words).generate(character_str)\n        plt.figure(figsize=(15, 10))\n        plt.imshow(wordcloud) \n        plt.axis(\"off\")\n        plt.savefig(f\"Word Cloud {character}.jpg\")\n        plt.title(character.title(), size = 80)\n        plt.show()\n        \nplot_wordclouds(character_related_sentences, main_characters)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nText Similarity Analysis\n\nSimilarity between various translations\n\n# Read four various translations of the epic in str and list formats\nmb_summary_1_list = read_file(file_mb_summary_1, preprocess = True, remove_stop_words = True, return_type = list)\nmb_summary_2_list = read_file(file_mb_summary_2, preprocess = True, remove_stop_words = True, return_type = list)\nmb_full_1_list = read_file(file_mb_full_1, preprocess = True, remove_stop_words = True, return_type = list)\nmb_full_2_list = read_file(file_mb_full_2, preprocess = True, remove_stop_words = True, return_type = list)\n\nmb_summary_1_str = read_file(file_mb_summary_1, preprocess = True, remove_stop_words = True, return_type = str)\nmb_summary_2_str = read_file(file_mb_summary_2, preprocess = True, remove_stop_words = True, return_type = str)\nmb_full_1_str = read_file(file_mb_full_1, preprocess = True, remove_stop_words = True, return_type = str)\nmb_full_2_str = read_file(file_mb_full_2, preprocess = True, remove_stop_words = True, return_type = str)\n\n\n# Print the similarity matrix that depicts similarities between all the translations\nvectorizer = TfidfVectorizer()\ntfidf = vectorizer.fit_transform([mb_full_1_str, mb_full_2_str, mb_summary_1_str, mb_summary_2_str])\nsimilarity = np.round(cosine_similarity(tfidf),2)\nnumbers = ['Full T1', 'Full T2', 'Summary T1', 'Summary T2']\ncentralities = ['Book', 'Full T1', 'Full T2', 'Summary T1', 'Summary T2']\nprint(tabulate(*[np.c_[numbers, similarity*100]], headers = centralities))\n\nBook          Full T1    Full T2    Summary T1    Summary T2\n----------  ---------  ---------  ------------  ------------\nFull T1           100         62            62            44\nFull T2            62        100            58            42\nSummary T1         62         58           100            63\nSummary T2         44         42            63           100\n\n\n\ndef plot_similarity(similarity_matrix):\n    \"\"\"\n    This function plots the similarity in a bar chart format \n    Parameters\n    ----------\n    similarity_matrix : list\n          list of similarity values\n    \"\"\" \n    fig, axes = plt.subplots(nrows = 1, ncols = 4, figsize=(25, 8))\n    ax = axes.flatten()\n    fig.suptitle('Text Similarity comparison using tf-idf and cosine similarity', size = 30, y = 1.05 )\n    book_names = np.array(['Ganguly', 'Bibek', 'Rajagopalachari', 'Narayanan'])\n    for i in range(len(similarity_matrix)):\n        ax[i].set_title(f'{book_names[i]}', size = 20)\n        plt.figure(figsize = (24, 9))\n        indices = list(range(4))\n        indices.pop(i)\n        vals = similarity_matrix[i][indices]\n        rank = vals.argsort().argsort()\n        keys = book_names[indices]\n        color_palette = np.array(sns.color_palette(\"Blues\", len(vals)))\n        g = sns.barplot(y=keys, x=vals, palette = color_palette[rank], alpha = 0.75, ax = ax[i])\n        g.bar_label(g.containers[0], labels = vals, size = 14)\n        g.set_yticklabels(keys, size = 20)\n        g.tick_params(axis='both', which='major', labelsize = 20)\n\n    fig.tight_layout()\n    fig.savefig('all_texts_similarity.jpg', )\n    fig.show()\nplot_similarity(similarity)\n\n/tmp/ipykernel_230/2508381481.py:29: UserWarning: Matplotlib is currently using module://matplotlib_inline.backend_inline, which is a non-GUI backend, so cannot show the figure.\n  fig.show()\n\n\n\n\n\n<Figure size 2400x900 with 0 Axes>\n\n\n<Figure size 2400x900 with 0 Axes>\n\n\n<Figure size 2400x900 with 0 Axes>\n\n\n<Figure size 2400x900 with 0 Axes>\n\n\n\nLet us now find chapterwise similarity between two translations of the Epic\n\n\nGanguli (FT-1) vs Bibek (FT-2)\n\n# Extract chapters from the Ganguli translation\nimport os\nganguli_chapters = dict()\nfile_mb_summary_1 = Path.cwd() / \"Mahabharata/Mahabharata-C.Rajagopalachari.txt\"\nfile_mb_summary_2 = Path.cwd() / \"Mahabharata/Mahabharata-R.K.Narayan.txt\"\nfile_mb_full_1 = \nfile_mb_full_2 = Path.cwd() / \"Mahabharata/Mahabharata-Bibek_Debroy.txt\"\n\nganguli = r\"E:\\Social Networks and Text Analytics\\CA2\\Mahabharata\\ganguli\"\nfor root, dirs, files in os.walk(ganguli, topdown=False):\n    for name in files:\n        if '.txt' not in name:\n            continue\n        t1 = read_file(os.path.join(root, name), remove_stop_words = True, return_type = list)\n        index = int(name.split('.')[0])\n        ganguli_chapters[index] = t1\n\n\n# Extract chapters from the Bibek translation\nbibek_chapters = dict()\nbibek = r\"E:\\Social Networks and Text Analytics\\CA2\\Mahabharata\\bibek\"\nfor root, dirs, files in os.walk(bibek, topdown=False):\n    for name in files:\n        if '.txt' not in name:\n            continue\n        t1 = read_file(os.path.join(root, name), remove_stop_words = True, return_type = list)\n        index = int(name.split('.')[0])\n        bibek_chapters[index] = t1\n\n\ndef get_cosine_similarity(doc_list):\n    \"\"\"\n    This function plots the similarity in a bar chart format \n    Parameters\n    ----------\n    doc_list : list\n          list of documents to compare similarity\n    Returns\n    -------\n    similarity : list\n        list of similarity values\n    \"\"\"\n    vectorizer = TfidfVectorizer()\n    tfidf = vectorizer.fit_transform(doc_list)\n    similarity = np.round(cosine_similarity(tfidf),2)\n    return similarity\n\n\n# Get Chapterwise Similarity\nchapter_wise_similarity = []\nfor i in range(1, 19):\n    a = get_cosine_similarity([' '.join(bibek_chapters[i]), ' '.join(ganguli_chapters[i])])\n    chapter_wise_similarity.append(a[0][1])\n\nKeyError: 1\n\n\n\nprint(chapter_wise_similarity)\n\n\n# Plot Chapterwise similarity between the above mentioned two translations\nfont_size = 45\nplt.figure(figsize = (25, 15))\nplt.xlabel('Chapters', fontsize = font_size)\nplt.ylabel('Text Similarity - Cosine', fontsize = font_size)\nplt.xticks(range(1,19))\n\ng = sns.barplot(x= np.arange(1,19), y = chapter_wise_similarity, palette = \"winter\", color = 'blue', alpha = 0.75)\ng.bar_label(g.containers[0], labels = chapter_wise_similarity, size = 35)\ng.tick_params(axis='x', which = 'major', labelsize = font_size)\ng.tick_params(axis='y', which = 'major', labelsize = font_size)\ng.set_title('Chapterwise similarity between FT1 and FT2', fontsize = font_size)\ng.set_ylim(0,0.65)\nplt.tight_layout()\nplt.savefig('Chapterwise-similarity.jpg')\n\n\nemotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'sadness', 'surprise', 'trust']\nemotion_intensity_file_path = \"NRC-Emotion-Intensity-Lexicon.txt\"\n\n\n\nLet us analyse sentiment of each chapter using the NRC Emotion Lexicon Wordlevel\n\nfile = \"NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n\ndef get_word_emotion_lexicon_wordlevel(emotions, emotion_intensity_file_path):\n    \"\"\"\n    This function plots the similarity in a bar chart format \n    Parameters\n    ----------\n    emotions : list\n          list of emotions that needs to be extracted from the lexicon\n    emotion_intensity_file_path : str\n          path to the NRC wordlevel emotion file    \n    Returns\n    -------\n    res : tuple of sets\n        Tuple of positive and negative words\n    \"\"\"\n    emotion_lexicon = []\n    positive = []\n    negative = []\n    with open(emotion_intensity_file_path) as file:\n        for line in file:\n            word_int_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n            if word_int_array[1] == 'negative' and word_int_array[2] == '1':\n                negative.append(word_int_array[0])\n            elif word_int_array[1] == 'positive' and word_int_array[2] == '1':\n                positive.append(word_int_array[0])\n    res = (set(positive), set(negative))\n    return res\n\n\npositive_words, negative_words = get_word_emotion_lexicon_wordlevel(emotions, file)\n\n\npositive_words\n\n\ndef get_word_sentiment_per_part(text, no_of_parts = 1):\n    \"\"\"\n    This function plots the similarity in a bar chart format \n    Parameters\n    ----------\n    text : list\n          list of emotions that needs to be extracted from the lexicon\n    no_of_parts : str\n          no of parts the text is to be split into.\n          This can be useful in case if the text does not \n          have defined chapters\n          \n    Returns\n    -------\n    res : tuple of sets\n        list that contains positive sentiments and negative sentiments \n        per each chapter\n        \n    \"\"\"\n    positive_word_count = []\n    negative_word_count = []\n    total_word_count = []\n    for sentence in text:\n        pc = 0\n        nc = 0\n        tc = 0\n        for word in sentence.split():\n            tc+=1\n            lemm_word = lemmatize_word(word)\n            if word in positive_words or lemm_word in positive_words:\n                pc+=1\n            elif word in negative_words or lemm_word in negative_words:\n                nc+=1\n        if tc == pc or tc == nc:\n            continue\n        positive_word_count.append(pc)\n        negative_word_count.append(nc)\n        total_word_count.append(tc)\n    pwc_split = np.array_split(np.array(positive_word_count), no_of_parts)\n    nwc_split = np.array_split(np.array(negative_word_count), no_of_parts)\n    twc_split = np.array_split(np.array(total_word_count), no_of_parts)\n    pos_res = []\n    neg_res = []\n    for i in range(len(pwc_split)):\n        pos_res.append(np.sum(pwc_split[i])/np.sum(twc_split[i]) )\n        neg_res.append(np.sum(nwc_split[i])/np.sum(twc_split[i]) ) \n    res = pos_res, neg_res\n    return res\n\n\nmb_full_1_list = read_file(file_mb_full_1, preprocess = True, remove_stop_words = False, return_type = list)\nmb_full_2_list = read_file(file_mb_full_2, preprocess = True, remove_stop_words = False, return_type = list)\nmb_summary_1_list = read_file(file_mb_summary_1, preprocess = True, remove_stop_words = False, return_type = list)\nmb_summary_1_list = read_file(file_mb_summary_1, preprocess = True, remove_stop_words = False, return_type = list)\n\n\n\nGet chapterwise sentiment of Ganguli and Bibek translations\n\n# Get chapterwise sentiments of Ganguli translation\nganguli_pos_1 = []\nganguli_neg_1 = []\nfor key, chapter in ganguli_chapters.items():\n    a, b = get_word_sentiment_per_part(chapter, no_of_parts = 1)\n    ganguli_pos_1.append(a)\n    ganguli_neg_1.append(b)\n\n\n# Get chapterwise sentiments of Bibek translation\nbibek_pos_1 = []\nbibek_neg_1 = []\nfor key, chapter in bibek_chapters.items():\n    a, b = get_word_sentiment_per_part(chapter, no_of_parts = 1)\n    bibek_pos_1.append(a)\n    bibek_neg_1.append(b)\n\n\nganguli_pos_1 = np.array(ganguli_pos_1).flatten()\nganguli_neg_1 = np.array(ganguli_neg_1).flatten()\nbibek_pos_1   = np.array(bibek_pos_1).flatten()\nbibek_neg_1   = np.array(bibek_neg_1).flatten()\n\n\n# Plot chapterwise sentiments of both Ganguli and Bibek translations \n# using NRC Emotion Lexicon Wordlevel\n\nfont_size = 40\nlegend_size = 30\ntitle_size = 50\nfig, ax = plt.subplots(2, figsize=(18, 20))\n\naxis_index = 0\nax[axis_index].set_title(f'Positive Sentiment', fontsize = title_size)\nax[axis_index].plot(np.arange(1,19), ganguli_pos_1, color = 'blue', marker='8', label = f'FT-1 Positive',  \n                    linewidth = 3, markersize=20, linestyle = '-')\nax[axis_index].plot(np.arange(1,19), bibek_pos_1, color = 'brown', marker='8', label = f'FT-2 Positive',  \n                    linewidth = 3, markersize=20, linestyle = '-')\n\nax[axis_index].set_xticks(np.arange(1,19))\nax[axis_index].set_xticklabels(np.arange(1,19), fontsize = font_size)\nax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\nax[axis_index].set_xlabel('Chapters', fontsize = font_size)\nax[axis_index].set_ylabel('Percentage', fontsize = font_size)\n\n\naxis_index = 1\nax[axis_index].set_title(f'Negative Sentiment', fontsize = title_size)\nax[axis_index].plot(np.arange(1,19), ganguli_neg_1, color = 'blue', marker='8', label = f'FT-1 Positive',  \n                    linewidth = 3, markersize=20, linestyle = '-')\nax[axis_index].plot(np.arange(1,19), bibek_neg_1, color = 'brown', marker='8', label = f'FT-2 Positive',  \n                    linewidth = 3, markersize=20, linestyle = '-')\n\nax[axis_index].set_xticks(np.arange(1,19))\nax[axis_index].set_xticklabels(np.arange(1,19), fontsize = font_size)\nax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\nax[axis_index].set_xlabel('Chapters', fontsize = font_size)\nax[axis_index].set_ylabel('Percentage', fontsize = font_size)\n\n\nplt.tight_layout()\nfig.subplots_adjust(hspace=.5)\nplt.savefig('Emotion-word-utterance.jpg')\n\nplt.show()\n\nUserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n<ipython-input-42-df851774c5fe>:32: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n\n\n\n\n\n\n# abcd = get_word_sentiment_per_part(bibek_chapters, no_of_parts = 18)\n\n\nimport nltk\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /home/yasaswi/nltk_data...\n\n\n\nsid.polarity_scores('happy good')\n\n{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.765}\n\n\n\ndef get_sentiment_scores(text_list):\n    \"\"\"\n    This function plots the similarity in a bar chart format \n    Parameters\n    ----------\n    text_list : list\n          List of sentences\n          \n    Returns\n    -------\n    res : \n        Returns the normalised VADER sentiment scores\n        Scores are normalised by dividing scores per chapter with the \n        no of sentences per chapter\n    \"\"\"\n    sentiment_scores_neg = 0.0\n    sentiment_scores_pos = 0.0\n    sentiment_scores_neu = 0.0\n    sentiment_scores_compound = 0.0\n    \n    for sentence in text_list:\n        res = sid.polarity_scores(sentence)\n        sentiment_scores_neg += res['neg']\n        sentiment_scores_pos += res['pos']\n        sentiment_scores_neu += res['neu']\n        sentiment_scores_compound += res['compound']\n    \n    sentiment_scores_neg = round(sentiment_scores_neg * 100 / len(text_list), 3)\n    sentiment_scores_pos = round(sentiment_scores_pos * 100 / len(text_list), 3)\n    sentiment_scores_neu = round(sentiment_scores_neu * 100 / len(text_list), 3)\n    sentiment_scores_compound = round(sentiment_scores_compound * 100 / len(text_list), 3)\n    res = sentiment_scores_neg, sentiment_scores_pos, sentiment_scores_neu, sentiment_scores_compound, \n    return res\n\n\n# Get VADER scores for Ganguli's Translation \nganguli_scores_neg = []\nganguli_scores_pos = []\nganguli_scores_neu = []\nganguli_scores_compound = []\n\nfor ch, text in ganguli_chapters.items():\n    a,b,c,d = get_sentiment_scores(text)\n    ganguli_scores_neg.append(a)\n    ganguli_scores_pos.append(b)\n    ganguli_scores_neu.append(c)\n    ganguli_scores_compound.append(d)\n\n\n# Get VADER scores for Bibek's Translation \nbibek_scores_neg = []\nbibek_scores_pos = []\nbibek_scores_neu = []\nbibek_scores_compound = []\n\nfor ch, text in bibek_chapters.items():\n    a,b,c,d = get_sentiment_scores(text)\n    bibek_scores_neg.append(a)\n    bibek_scores_pos.append(b)\n    bibek_scores_neu.append(c)\n    bibek_scores_compound.append(d)\n\n\n# Plot chapterwise sentiments of both Ganguli and Bibek translations \n# using VADER sentiment scores\n\nfont_size = 40\nlegend_size = 30\ntitle_size = 50\nfig, ax = plt.subplots(2, figsize=(18, 20))\n\naxis_index = 0\nax[axis_index].set_title(f'Positive Sentiment - VADER', fontsize = title_size)\nax[axis_index].plot(np.arange(1,19), np.array(ganguli_scores_pos)/100, color = 'blue', marker='8', label = f'FT-1 Positive',  \n                    linewidth = 3, markersize=20, linestyle = '-')\nax[axis_index].plot(np.arange(1,19), np.array(bibek_scores_pos)/100, color = 'brown', marker='8', label = f'FT-2 Positive',  \n                    linewidth = 3, markersize=20, linestyle = '-')\nax[axis_index].set_xticks(np.arange(1,19))\nax[axis_index].set_xticklabels(np.arange(1,19), fontsize = font_size)\nax[axis_index].set_xlabel('Chapters', fontsize = font_size)\nax[axis_index].set_ylabel('Percentage', fontsize = font_size)\nax[axis_index].tick_params(axis='both', which='major', labelsize= font_size)\n\naxis_index = 1\nax[axis_index].set_title(f'Negative Sentiment - VADER', fontsize = title_size)\nax[axis_index].plot(np.arange(1,19), np.array(ganguli_scores_neg)/100, color = 'blue', marker='8', label = f'FT-1 Positive',  \n                    linewidth = 3, markersize=20, linestyle = '-')\nax[axis_index].plot(np.arange(1,19), np.array(bibek_scores_neg)/100, color = 'brown', marker='8', label = f'FT-2 Positive',  \n                    linewidth = 3, markersize=20, linestyle = '-')\n\nax[axis_index].set_xticks(np.arange(1,19))\nax[axis_index].set_xticklabels(np.arange(1,19), fontsize = font_size)\nax[axis_index].set_xlabel('Chapters', fontsize = font_size)\nax[axis_index].set_ylabel('Percentage', fontsize = font_size)\nax[axis_index].tick_params(axis='both', which='major', labelsize= font_size)\nplt.tight_layout()\nfig.subplots_adjust(hspace=.5)\nplt.savefig('valer-sentiment.jpg')\n\nplt.show()\n\n\n\n\n\ndef get_word_emotion_intensity_lexicon(emotions, emotion_intensity_file_path):\n    emotion_lexicon = []\n    word_intensities = {'anger':dict(),\n                        'anticipation':dict(),\n                        'disgust':dict(),\n                        'fear':dict(),\n                        'joy':dict(),\n                        'sadness':dict(),\n                        'surprise':dict(),\n                        'trust':dict()}\n    with open(emotion_intensity_file_path) as file:\n        for line in file:\n            word_int_array = line.replace(\"\\n\", \"\").split(\"\\t\")\n            emotion = word_int_array[1]\n            emotion_lexicon.append(word_int_array[0])\n            if emotion not in word_intensities.keys():\n                continue\n            word_intensities[emotion][word_int_array[0]] = float(word_int_array[2])\n    return word_intensities, emotion_lexicon\n\n\ndef get_emotion_intensity(text):\n    \"\"\"\n    This function extracts the emotion intensities from text  \n    Parameters\n    ----------\n    text : str\n          string from which emotion intensities are to be calculated\n          \n    Returns\n    -------\n    res : \n        Returns the emotion intensity scoress - actual and normalised\n        scores of all emotions calculated using text\n    \"\"\"\n    emotions_word_count = np.zeros(shape = (len(emotions)))\n    emotions_intensity = np.zeros(shape = (len(emotions)))\n    wc = 0\n    for word in text.split():\n        wc += 1\n        lemm_word = lemmatize_word(word)\n        if word in emotion_lexicon:\n            target = word\n        elif lemm_word in emotion_lexicon:\n            target = lemm_word\n        else:\n            continue\n\n        if target in word_intensities['anger'].keys():\n                emotions_intensity[0] += word_intensities['anger'][target]\n                emotions_word_count[0]+=1\n\n        if target in word_intensities['anticipation'].keys():\n                emotions_intensity[1] += word_intensities['anticipation'][target]\n                emotions_word_count[1]+=1\n\n        if target in word_intensities['disgust'].keys():\n                emotions_intensity[2] += word_intensities['disgust'][target]\n                emotions_word_count[2]+=1\n\n        if target in word_intensities['fear'].keys():\n                emotions_intensity[3] += word_intensities['fear'][target]\n                emotions_word_count[3]+=1\n\n        if target in word_intensities['joy'].keys():\n                emotions_intensity[4] += word_intensities['joy'][target]\n                emotions_word_count[4]+=1\n\n        if target in word_intensities['sadness'].keys():\n                emotions_intensity[5] += word_intensities['sadness'][target]\n                emotions_word_count[5]+=1\n\n        if target in word_intensities['surprise'].keys():\n                emotions_intensity[6] += word_intensities['surprise'][target]\n                emotions_word_count[6]+=1\n\n        if target in word_intensities['trust'].keys():\n                emotions_intensity[7] += word_intensities['trust'][target]\n                emotions_word_count[7]+=1\n    res = emotions_intensity, emotions_intensity / emotions_word_count\n    return res\n\n\nword_intensities, emotion_lexicon = get_word_emotion_intensity_lexicon(emotions, emotion_intensity_file_path)\n\n\n# Get Emotion Intensity scores of the two full translations of the Epic\nres3, res_norm3 = get_emotion_intensity(mb_full_1_str)\nres4, res_norm4 = get_emotion_intensity(mb_full_2_str)\n\n\n\n\nPlot Emotion Intensity scores of full translations of the Epic - Full Translation 1 vs Full Translation 2\n\ndf = pd.DataFrame([res3, res4], \n                  columns = emotions, index = ['Full-1', 'Full-2',]).round(2).T\n\ndf['Emotions'] = df.index\nfont_size = 35\ng = df.plot(x = 'Emotions',\n        kind = 'bar',\n        stacked = False,\n        figsize = (25, 10),\n        rot = 0,\n        fontsize = font_size,\n        width = 0.7\n       )\nlegend_properties = {'weight':'bold'}\ng.legend(bbox_to_anchor=(1, 1), fontsize = font_size)\nplt.xlabel('Emotions' , fontsize = font_size,)\nplt.ylabel('Intensity Score', fontsize = font_size, )\ng.tick_params(axis='x', which = 'major', labelsize = font_size, rotation = 20)\ng.tick_params(axis='y', which = 'major', labelsize = font_size)\n\nplt.title(label = 'Total Emotion Intensity in four translations of the Mahabharata', \n          size = font_size, **{'fontname':'sans-serif', 'fontweight': '649'})\n# plt.ylim(0,65)\nplt.tight_layout()\nplt.savefig('Emotion Intensities.jpg')\nplt.show()\n\n\n\n\n\ndef get_emotion_intensity(text):\n    \"\"\"\n    This function returns emotion intensities from the text. \n    Parameters\n    ----------\n    text : str\n          string from which emotion intensities are to be calculated\n          \n    Returns\n    -------\n    res : \n        Returns the emotion intensity scoress - actual and normalised\n        scores of all emotions calculated using text\n    \"\"\"\n    emotions_word_count = np.zeros(shape = (len(emotions)))\n    emotions_intensity = np.zeros(shape = (len(emotions)))\n\n    wc = 0\n    for word in text.split():\n        wc += 1\n        lemm_word = lemmatize_word(word)\n        if word in emotion_lexicon:\n            target = word\n        elif lemm_word in emotion_lexicon:\n            target = lemm_word\n        else:\n            continue\n\n        if target in word_intensities['anger'].keys():\n                emotions_intensity[0] += word_intensities['anger'][target]\n                emotions_word_count[0]+=1\n\n        if target in word_intensities['anticipation'].keys():\n                emotions_intensity[1] += word_intensities['anticipation'][target]\n                emotions_word_count[1]+=1\n\n        if target in word_intensities['disgust'].keys():\n                emotions_intensity[2] += word_intensities['disgust'][target]\n                emotions_word_count[2]+=1\n\n        if target in word_intensities['fear'].keys():\n                emotions_intensity[3] += word_intensities['fear'][target]\n                emotions_word_count[3]+=1\n\n        if target in word_intensities['joy'].keys():\n                emotions_intensity[4] += word_intensities['joy'][target]\n                emotions_word_count[4]+=1\n\n        if target in word_intensities['sadness'].keys():\n                emotions_intensity[5] += word_intensities['sadness'][target]\n                emotions_word_count[5]+=1\n\n        if target in word_intensities['surprise'].keys():\n                emotions_intensity[6] += word_intensities['surprise'][target]\n                emotions_word_count[6]+=1\n\n        if target in word_intensities['trust'].keys():\n                emotions_intensity[7] += word_intensities['trust'][target]\n                emotions_word_count[7]+=1\n    res = (emotions_intensity, emotions_intensity / emotions_word_count)\n    return res\n\n\ndef get_chapterwise_emotion_intensities(text_chapterwise):\n    \"\"\"\n    This function returns chapterwise intensities the similarity in a bar chart format \n    Parameters\n    ----------\n    text : str\n          string from which emotion intensities are to be calculated\n          \n    Returns\n    -------\n    res : \n        Returns the emotion intensity scoress - actual and normalised\n        scores of all emotions calculated using text\n    \"\"\"\n    res = []\n    for i in range(1, 19):\n        a = get_emotion_intensity(' '.join(text_chapterwise[i]))[1]\n        res.append(a)\n    return res\n\ndef extract_emotion_intensity(chapterwise_emotions, emotion):\n    \"\"\"\n    This function plots the similarity in a bar chart format \n    Parameters\n    ----------\n    text : str\n          string from which emotion intensities are to be calculated\n          \n    Returns\n    -------\n    res : \n        Returns the emotion intensity scoress - actual and normalised\n        scores of all emotions calculated using text\n    \"\"\"\n    index = {'anger': 0, 'anticipation': 1, 'disgust': 2, 'fear': 3, 'joy': 4, 'sadness': 5, 'surprise': 6, 'trust': 7}\n    res = []\n    for emotion_list in chapterwise_emotions:\n        res.append(emotion_list[index[emotion]])\n    return np.array(res)\n\n\nganguli_chapterwise_emotions = get_chapterwise_emotion_intensities(ganguli_chapters)\n\n\nbibek_chapterwise_emotions = get_chapterwise_emotion_intensities(bibek_chapters)\n\n\n\nChapterwise Emotion Intensity of Anger, Disgust, Fear and Sadness\n\n# plt.figure(figsize=(6, 4))\nfont_size = 30\nlegend_size = 30\ntitle_size = 40\nfig, ax = plt.subplots(4, figsize=(14, 25))\n\naxis_index = 0\n\naxes = [0, 2, 3, 5] # index positions of Anger, Disgust, Fear and Sadness \nfor axis_index, i, in enumerate(axes):\n    text1 = extract_emotion_intensity(ganguli_chapterwise_emotions, emotions[i])\n    text2 = extract_emotion_intensity(bibek_chapterwise_emotions, emotions[i])\n    ax[axis_index].plot(np.arange(1,19), text2, color = 'blue', marker='o', label = f'FT1-{emotions[i]}',  linewidth =3, \n                    markersize=20, linestyle = '-')\n    ax[axis_index].plot(np.arange(1,19), text1, color = 'brown', marker='o', label = f'FT1-{emotions[i]}',  linewidth =3, \n                    markersize=20, linestyle = '-')\n    ax[axis_index].set_title(f'\"{emotions[i].title()}\" Intensity ', fontsize = title_size)\n    ax[axis_index].set_xticks(np.arange(1,19))\n    ax[axis_index].set_xticklabels(np.arange(1,19), fontsize = font_size)\n    ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n    ax[axis_index].set_xlabel('Chapters', fontsize = font_size)\n    ax[axis_index].set_ylabel('Emotion Intensity', fontsize = font_size)\n    # legend = ax[axis_index].legend(loc='best', shadow=True, fontsize = legend_size)\n\n\nplt.tight_layout()\nfig.subplots_adjust(hspace=.5)\nplt.savefig('negative_emotions.jpg')\n\nplt.show()\n\nUserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n<ipython-input-60-56a6fdb1e5c6>:20: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n<ipython-input-60-56a6fdb1e5c6>:20: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n<ipython-input-60-56a6fdb1e5c6>:20: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n\n\n\n\n\n\n\nChapterwise Emotion Intensity of Anticipation, Joy, Surprise and Trust\n\n# plt.figure(figsize=(6, 4))\nfont_size = 30\nlegend_size = 30\ntitle_size = 40\nfig, ax = plt.subplots(4, figsize=(14, 25))\n\naxis_index = 0\n\na = [1, 4, 6, 7] # index positions of anticipation, joy, surprise and trust\nfor axis_index, i, in enumerate(a):\n    text2 = extract_emotion_intensity(bibek_chapterwise_emotions, emotions[i])\n    text1 = extract_emotion_intensity(ganguli_chapterwise_emotions, emotions[i])\n    ax[axis_index].plot(np.arange(1,19), text2, color = 'blue', marker='o', label = f'FT1-{emotions[i]}',  linewidth =3, \n                    markersize=20, linestyle = '-')\n    ax[axis_index].plot(np.arange(1,19), text1, color = 'brown', marker='o', label = f'FT1-{emotions[i]}',  linewidth =3, \n                    markersize=20, linestyle = '-')\n    ax[axis_index].set_title(f'\"{emotions[i].title()}\" Intensity ', fontsize = title_size)\n    ax[axis_index].set_xticks(np.arange(1,19))\n    ax[axis_index].set_xticklabels(np.arange(1,19), fontsize = font_size)\n    ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n    ax[axis_index].set_xlabel('Chapters', fontsize = font_size)\n    ax[axis_index].set_ylabel('Emotion Intensity', fontsize = font_size)\n  # legend = ax[axis_index].legend(loc='best', shadow=True, fontsize = legend_size)\n\n\nplt.tight_layout()\nfig.subplots_adjust(hspace=.5)\nplt.savefig('positive_emotions.jpg')\n\nplt.show()\n\nUserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n<ipython-input-61-927b2cae210f>:20: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n<ipython-input-61-927b2cae210f>:20: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )\n<ipython-input-61-927b2cae210f>:20: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax[axis_index].set_yticklabels(np.arange(0.0, 0.9, 0.1, dtype = np.float32), fontsize = font_size, )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "my_blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 11, 2023\n\n\nTristan OâMalley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnât specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]